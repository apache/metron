#
#  Licensed to the Apache Software Foundation (ASF) under one or more
#  contributor license agreements.  See the NOTICE file distributed with
#  this work for additional information regarding copyright ownership.
#  The ASF licenses this file to You under the Apache License, Version 2.0
#  (the "License"); you may not use this file except in compliance with
#  the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#
---
# vars file for single_node_vm blueprint

hadoop_master: [NAMENODE, SECONDARY_NAMENODE, RESOURCEMANAGER, HISTORYSERVER]
hadoop_slave: [APP_TIMELINE_SERVER, DATANODE, HDFS_CLIENT, NODEMANAGER, YARN_CLIENT, MAPREDUCE2_CLIENT]
spark_master: [SPARK_JOBHISTORYSERVER]
spark_slave: [SPARK_CLIENT]
storm_master: [NIMBUS, STORM_UI_SERVER, DRPC_SERVER]
storm_slave: [SUPERVISOR]
kafka_broker: [KAFKA_BROKER]
zookeeper_master: [ZOOKEEPER_SERVER]
zookeeper_slave: [ZOOKEEPER_CLIENT]
hbase_master: [HBASE_MASTER, HBASE_CLIENT]
hbase_slave: [HBASE_REGIONSERVER]

metron_components: "{{ hadoop_master | union(zookeeper_master) | union(storm_master) | union(hbase_master) | union(hadoop_slave) | union(zookeeper_slave) | union(storm_slave) | union(kafka_broker) | union(hbase_slave) }}"

cluster_name: "metron_cluster"
blueprint_name: "metron_blueprint"

configurations:
  - zoo.cfg:
      dataDir: '{{ zookeeper_data_dir }}'
  - hadoop-env:
      hadoop_heapsize: 1024
      namenode_heapsize: 512
      dtnode_heapsize: 512
      namenode_opt_permsize: 128m
  - hbase-env:
      hbase_regionserver_heapsize: 512
      hbase_master_heapsize: 512
      hbase_regionserver_xmn_max: 512
  - hdfs-site:
      dfs.namenode.checkpoint.dir: '{{ namenode_checkpoint_dir  }}'
      dfs.namenode.name.dir: '{{ namenode_name_dir }}'
      dfs.datanode.data.dir: '{{ datanode_data_dir }}'
      dfs.journalnode.edits.dir: '{{ journalnode_edits_dir }}'
  - yarn-env:
      nodemanager_heapsize: 512
      yarn_heapsize: 512
      apptimelineserver_heapsize : 512
      resourcemanager_heapsize: 1024
  - mapred-env:
      jobhistory_heapsize: 256
  - mapred-site:
      mapreduce.jobhistory.recovery.store.leveldb.path : '{{ jhs_recovery_store_ldb_path }}'
      mapreduce.map.memory.mb : '{{ mapred_map_mem_mb }}'
      mapreduce.reduce.memory.mb : '{{ mapred_reduce_mem_mb }}'
  - yarn-site:
      yarn.nodemanager.local-dirs : '{{ nodemanager_local_dirs }}'
      yarn.timeline-service.leveldb-timeline-store.path: '{{ timeline_ldb_store_path }}'
      yarn.timeline-service.leveldb-state-store.path: '{{ timeline_ldb_state_path }}'
      yarn.nodemanager.log-dirs: '{{ nodemanager_log_dirs }}'
      yarn.nodemanager.resource.memory-mb : '{{ nodemanager_mem_mb }}'
  - storm-site:
      supervisor.slots.ports: "[6700, 6701, 6702, 6703]"
      storm.local.dir: '{{ storm_local_dir }}'
  - kafka-env:
      content: "{% raw %}\n#!/bin/bash\n\n# Set KAFKA specific environment variables here.\n\n# The java implementation to use.\nexport KAFKA_HEAP_OPTS=\"-Xms256M -Xmx256M\"\nexport KAFKA_JVM_PERFORMANCE_OPTS=\"-server -XX:+UseG1GC -XX:+DisableExplicitGC -Djava.awt.headless=true\"\nexport JAVA_HOME={{java64_home}}\nexport PATH=$PATH:$JAVA_HOME/bin\nexport PID_DIR={{kafka_pid_dir}}\nexport LOG_DIR={{kafka_log_dir}}\nexport KAFKA_KERBEROS_PARAMS={{kafka_kerberos_params}}\n# Add kafka sink to classpath and related depenencies\nif [ -e \"/usr/lib/ambari-metrics-kafka-sink/ambari-metrics-kafka-sink.jar\" ]; then\n  export CLASSPATH=$CLASSPATH:/usr/lib/ambari-metrics-kafka-sink/ambari-metrics-kafka-sink.jar\n  export CLASSPATH=$CLASSPATH:/usr/lib/ambari-metrics-kafka-sink/lib/*\nfi\nif [ -f /etc/kafka/conf/kafka-ranger-env.sh ]; then\n   . /etc/kafka/conf/kafka-ranger-env.sh\nfi{% endraw %}"
  - kafka-broker:
      log.dirs: '{{ kafka_log_dirs }}'
      delete.topic.enable: "true"

blueprint:
  stack_name: HDP
  stack_version: 2.3
  groups:
    - name : host_group_1
      cardinality: 1
      configurations: []
      components: "{{ metron_components }}"
      hosts: "{{ hdp_host_group }}"
