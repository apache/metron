#
#  Licensed to the Apache Software Foundation (ASF) under one or more
#  contributor license agreements.  See the NOTICE file distributed with
#  this work for additional information regarding copyright ownership.
#  The ASF licenses this file to You under the Apache License, Version 2.0
#  (the "License"); you may not use this file except in compliance with
#  the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#
---

hadoop_master: [NAMENODE, SECONDARY_NAMENODE, RESOURCEMANAGER, HISTORYSERVER]
hadoop_slave: [APP_TIMELINE_SERVER, DATANODE, NODEMANAGER]
spark_master: [SPARK_JOBHISTORYSERVER]
storm_master: [NIMBUS, STORM_UI_SERVER, DRPC_SERVER]
storm_slave: [SUPERVISOR]
kafka_broker: [KAFKA_BROKER]
zookeeper_master: [ZOOKEEPER_SERVER]
hbase_master: [HBASE_MASTER]
hbase_slave: [HBASE_REGIONSERVER]
hadoop_clients: [HDFS_CLIENT, YARN_CLIENT, MAPREDUCE2_CLIENT, SPARK_CLIENT, ZOOKEEPER_CLIENT, HBASE_CLIENT]

master_1_components: "{{ hadoop_master | union(hadoop_clients) }}"
master_1_host:
  - "{{groups.ambari_slave[0]}}"
master_2_components: "{{ zookeeper_master | union(storm_master) | union(spark_master) | union(hbase_master) | union(hadoop_clients) }}"
master_2_host:
  - "{{groups.ambari_slave[1]}}"
slave_components: "{{ hadoop_slave | union(storm_slave) | union(kafka_broker) | union(hbase_slave) | union(hadoop_clients) }}"

cluster_name: "metron"
blueprint_name: "metron_blueprint"

configurations:
  - zoo.cfg:
      dataDir: '{{ zookeeper_data_dir | default("/hadoop/zookeeper") }}'
  - hdfs-site:
      dfs.namenode.checkpoint.dir: '{{ namenode_checkpoint_dir | default("/hadoop/hdfs/namesecondary") }}'
      dfs.namenode.name.dir: '{{ namenode_name_dir | default("/hadoop/hdfs/namenode") }}'
      dfs.datanode.data.dir: '{{ datanode_data_dir | default("/hadoop/hdfs/data" ) }}'
      dfs.journalnode.edits.dir: '{{ journalnode_edits_dir | default("/hadoop/hdfs/journalnode") }}'
  - hadoop-env:
      namenode_heapsize: 1024
      dtnode_heapsize: 1024
  - hbase-env:
      hbase_regionserver_heapsize: 1024
      hbase_master_heapsize: 1024
  - yarn-env:
      nodemanager_heapsize: 512
      yarn_heapsize: 512
      apptimelineserver_heapsize : 512
  - mapred-env:
      jobhistory_heapsize: 256
  - yarn-site:
      yarn.nodemanager.resource.memory-mb: 1024
      yarn.scheduler.maximum-allocation-mb: 1024
      yarn.nodemanager.local-dirs : '{{ nodemanager_local_dirs| default("/hadoop/yarn/local") }}'
      yarn.timeline-service.leveldb-timeline-store.path: '{{ timeline_ldb_store_path | default("/hadoop/yarn/timeline") }}'
      yarn.timeline-service.leveldb-state-store.path: '{{ timeline_ldb_state_path| default("/hadoop/yarn/timeline") }}'
      yarn.nodemanager.log-dirs: '{{ nodemanager_log_dirs| default("/hadoop/yarn/log") }}'

  - mapred-site:
      mapreduce.jobhistory.recovery.store.leveldb.path : '{{ jhs_recovery_store_ldb_path | default("/hadoop/mapreduce/jhs") }}'
  - storm-site:
      supervisor.slots.ports: "[6700, 6701, 6702, 6703]"
      storm.local.dir: '{{ storm_local_dir | default("/hadoop/storm") }}'
  - kafka-env:
      content: "{% raw %}\n#!/bin/bash\n\n# Set KAFKA specific environment variables here.\n\n# The java implementation to use.\nexport KAFKA_HEAP_OPTS=\"-Xms256M -Xmx256M\"\nexport KAFKA_JVM_PERFORMANCE_OPTS=\"-server -XX:+UseG1GC -XX:+DisableExplicitGC -Djava.awt.headless=true\"\nexport JAVA_HOME={{java64_home}}\nexport PATH=$PATH:$JAVA_HOME/bin\nexport PID_DIR={{kafka_pid_dir}}\nexport LOG_DIR={{kafka_log_dir}}\nexport KAFKA_KERBEROS_PARAMS={{kafka_kerberos_params}}\n# Add kafka sink to classpath and related depenencies\nif [ -e \"/usr/lib/ambari-metrics-kafka-sink/ambari-metrics-kafka-sink.jar\" ]; then\n  export CLASSPATH=$CLASSPATH:/usr/lib/ambari-metrics-kafka-sink/ambari-metrics-kafka-sink.jar\n  export CLASSPATH=$CLASSPATH:/usr/lib/ambari-metrics-kafka-sink/lib/*\nfi\nif [ -f /etc/kafka/conf/kafka-ranger-env.sh ]; then\n   . /etc/kafka/conf/kafka-ranger-env.sh\nfi{% endraw %}"
  - kafka-broker:
      log.dirs: '{{ kafka_log_dirs | default("/kafka-log") }}'

blueprint:
  stack_name: HDP
  stack_version: 2.3
  groups:
    - name : master_1
      cardinality: 1
      configuration: []  # configuration not yet implemented
      components: "{{ master_1_components }}"
      hosts: "{{ master_1_host }}"
    - name : master_2
      cardinality: 1
      configuration: []  # configuration not yet implemented
      components: "{{ master_2_components }}"
      hosts: "{{ master_2_host }}"
    - name: slaves
      cardinality: 1+
      configuration: []  # configuration not yet implemented
      components: "{{ slave_components }}"
      hosts: "{{ groups.ambari_slave | difference(groups.ambari_slave[0]) | difference(groups.ambari_slave[1]) }}"
